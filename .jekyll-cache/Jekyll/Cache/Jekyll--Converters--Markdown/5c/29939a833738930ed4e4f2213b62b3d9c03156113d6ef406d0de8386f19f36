I",Y<h1 id="机器学习期末复习总结">机器学习期末复习总结</h1>

<h2 id="第二章-基本术语和模型评估">第二章 基本术语和模型评估</h2>

<h3 id="任务">任务</h3>

<p>分类任务：标记为离散值</p>

<p>回归任务：标记为连续纸</p>

<p>聚类任务：标记为空值</p>

<h3 id="预测任务根据标记的完整情况">预测任务：根据标记的完整情况</h3>

<p>监督学习：所有示例有标记，分类、回归</p>

<p>无监督学习：所有实例没有标记，聚类</p>

<p>半监督学习：少量有标记，大量没有标记。</p>

<p>噪音标记学习：有标记，但不完全准确</p>

<h3 id="概念学习">概念学习</h3>

<p>假设空间：所有可能属性的组合</p>

<p>版本空间：与训练集一致的“假设集合”</p>

<p>归纳偏好：学习过程中对某种类型假设的偏好称作归纳偏好</p>

<p>No Free Lunch：总误差与学习算法无关</p>

<h3 id="模型评估与选择">模型评估与选择</h3>

<p>学习器在训练集上的误差称为训练误差或经验误差，在新样本上的误差称为测试误差或泛化误差。</p>

<h4 id="评估方法">评估方法：</h4>

<p>留出法：将数据集划分为两个互斥的集合</p>

<p>交叉验证法：将数据集划分为k个互斥子集，每次用k-1个子集的并作为训练集，余下的作为测试集，k常取10</p>

<p>自助法：从数据集有放回的随机采样m次、</p>

<p>自助法：数据集小，难以划分训练、测试集很有用；产生多个训练集，对集成学习有用</p>

<p>数据量足够，一般采用留出法和交叉验证</p>

<h4 id="性能度量">性能度量</h4>

<p>均方误差（MSE），错误率、精度，P，R，ROC(AUC)，$F_\beta$度量，代价敏感错误率。</p>

<p>ROC：</p>

<p><img src="C:\Users\赵超懿\AppData\Roaming\Typora\typora-user-images\image-20211229214655050.png" alt="image-20211229214655050" /></p>

<p>P：(Precision)查准率$\dfrac{TP}{TP+FP}$，预测结果正例中真实情况为正例占的比例</p>

<p>R：(Recall)查全率$\dfrac{TP}{TP+FN}$，真实情况正例中预测结果为正例占的比例</p>

<p>P-R曲线：P=R，平衡点，可用来度量P-R曲线有交叉的分类器性能的高低</p>

<p>$F_{\beta}=\dfrac{(1+\beta^2)\times P\times R}{(\beta^2\times P)+R}$,$F_\beta$度量，$\beta=1$，标准$F_1$度量。</p>

<p>比较检验（比较评价两个模型）：假设检验：二项检验、T-检验</p>

<p>偏差与方差</p>

<p>泛化误差可分解为方差、偏差与噪声之和</p>

<h2 id="第三章-线性模型">第三章 线性模型</h2>

<h3 id="回归任务掌握">回归任务（掌握）</h3>

<p>最小二乘法原理和推导</p>

<h3 id="二分类任务">二分类任务</h3>

<p>对数几率回归、线性判别分析的建模原理</p>

<p><img src="C:\Users\赵超懿\AppData\Roaming\Typora\typora-user-images\image-20211224160452914.png" alt="image-20211224160452914" /></p>

<p>线性判别分析</p>

<p><img src="C:\Users\赵超懿\AppData\Roaming\Typora\typora-user-images\image-20211224160508583.png" alt="image-20211224160508583" /></p>

<h3 id="多分类任务">多分类任务</h3>

<p>一对一</p>

<p>一对其余</p>

<p>多对多</p>

<h3 id="类别不平衡任务">类别不平衡任务</h3>

<p>欠采样、过采样、阈值移动</p>

<h2 id="第四章-决策树">第四章 决策树</h2>

<h3 id="决策树基本流程">决策树基本流程</h3>

<p>掌握决策树基本流程和原理</p>

<h4 id="基本流程">基本流程</h4>

<p>递归过程，当以下条件停止：</p>

<p>（1）当前节点包含的样本全部属于同一类别</p>

<p>（2）当前属性集为空，或所有样本在所有树性上取值相同</p>

<p>（3）当前节点包含的样本集合为空</p>

<h3 id="决策树算法的关键划分选择">决策树算法的关键：划分选择</h3>

<p>熟悉三种划分准则</p>

<h4 id="划分选择-信息增益">划分选择-信息增益</h4>

<p>信息熵：样本集合D中第k类样本所占的比例为$p_k,(k=1,2,…,|y|)$.
$
Ent(D)=-\sum\limits_{k=1}^{|y|}p_k\log_2{p_k}
$
$Ent(D)$表示集合的纯度  越小越纯</p>

<h4 id="信息增益">信息增益</h4>

<p>离散属性$a$，可能取值有$V$类，产生V个分支节点，第$v$个分支节点包含了D中所有在属性$a$上取值为$a^v$的样本，记为$D^v$。计算用属性a
$
Gain(D,a)=Ent(D)-\sum\limits_{v=1}^V\dfrac{|D^v|}{|D|}Ent(D^v)
$
选择$Gain(D,a)$最大的属性$a$来进行划分。</p>

<p>不足：若编号作为一个属性，信息增益一般远大于其他属性。信息增益指标偏好取值数目较多的属性</p>

<h4 id="基尼指数cart决策树">基尼指数——CART决策树</h4>

<p>C4.5：使用增益率，用属性的取值范围对信息增益做一个规范化。</p>

<p>基尼指数：$Gini(D)=1-\sum\limits_{k=1}^{|y|}p_k^2$.越小越纯</p>

<p>$Gini_index(D,a)=\sum\limits_{v=1}^V\dfrac{|D^v|}{|D|}Gini(D^v)$.选择使得$Gini_index(D,a)$最小的属性</p>

<h3 id="克服过拟合的问题剪枝处理">克服过拟合的问题：剪枝处理</h3>

<p>预剪枝 vs 后剪枝</p>

<p>原因：决策树容易过拟合</p>

<p>预剪枝：决策树生成过程中，对每个结点在划分前先进⾏估计，若当前结点的划分不能带来决策树泛化性能提升，则停⽌划分并将当前结点记为叶结点，其类别标记为训练样例数最多的类别。（边建树，边剪枝）（留出法，用一部分进行验证）</p>

<p>优点：降低过拟合风险，减少时间开销。缺点：欠拟合风险。</p>

<p>后剪枝：先建树，后剪枝</p>

<p>优点：欠拟合风险小 缺点：时间开销大</p>

<h3 id="处理多种类型数据连续与缺失值">处理多种类型数据：连续与缺失值</h3>

<p>了解基本原理</p>

<p>连续：连续属性离散化</p>

<p>缺失：Q1 划分 考虑一个属性时，仅使用当前属性无缺失样本学习</p>

<p>Q2：划分后样本的处理：将样本划分到每一个分支，赋予不同的权重</p>

<h3 id="决策树的变体多变量决策树">决策树的变体：多变量决策树</h3>

<p>了解基本原理</p>

<p>每一个非叶节点是一个线性分类器</p>

<h2 id="第五章-神经网络">第五章 神经网络</h2>

<h3 id="神经元模型熟悉">神经元模型：熟悉</h3>

<p>输入：来自其他n个神经云传递过来的输入信号。</p>

<p>处理：输入信号通过带权重的连接进行传递，神经元接收到总输入值将与神经元的阈值进行比较。</p>

<p>输出：通过激活函数的处理以得到输出。</p>

<h3 id="感知机与多层网络熟悉">感知机与多层网络：熟悉</h3>

<h4 id="感知机">感知机</h4>

<p>两层神经元组成，只能处理线性可分问题。</p>

<h4 id="多层前馈神经网络">多层前馈神经网络</h4>

<p>定义：每层神经元与下一层神经元全互联, 神经元 之间不存在同层连接也不存在跨层连接。</p>

<p>前馈：输入层接受外界输入, 隐含层与输出层神经 元对信号进行加工, 最终结果由输出层神经元输出。</p>

<h4 id="多层前馈网络表示能力">多层前馈网络表示能力</h4>

<p>只需要一个包含足够多神经元的隐含层，多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数。</p>

<h4 id="多层前馈网络局限">多层前馈网络局限</h4>

<p>容易过拟合</p>

<p>如何设置隐层神经元的个数仍然是个未决问题</p>

<h4 id="缓解过拟合策略">缓解过拟合策略</h4>

<p>早停：在训练中，若训练误差降低，但验证误差升高，则停止训练。</p>

<p>正则化：在误差目标函数中增加一项描述网络复杂程度的部分，例如连接权重与阈值的平方和。($E=\lambda\dfrac{1}{m}\sum\limits_{k=1}^mE_k+(1-\lambda)\sum\limits_iw_i^2$)</p>

<p><img src="C:\Users\赵超懿\AppData\Roaming\Typora\typora-user-images\image-20211225143040312.png" alt="image-20211225143040312" /></p>

<h3 id="误差逆传播算法熟悉">误差逆传播算法：熟悉</h3>

<h4 id="bp算法">BP算法</h4>

<p><img src="C:\Users\赵超懿\AppData\Roaming\Typora\typora-user-images\image-20211225143325524.png" alt="image-20211225143325524" /></p>

<p>推导注意链式法则，计算输入层的变化时要对所有的$\beta_j$分别链式法则求和。</p>

<h5 id="标准bp算法">标准BP算法</h5>

<p>每次针对单个训练样例更新权值与阈值。</p>

<p>参数更新频繁，不同样例可能抵消们需要多次迭代。</p>

<h5 id="累计bp算法">累计BP算法</h5>

<p>优点：其优化的目标是最小化整个训练集上的累计误差$E=\dfrac{1}{m}\sum\limits_{k=1}^mE_k$.</p>

<p>缺点：读取整个训练集一遍才对参数进行更新，参数更新频率较低。</p>

<h5 id="实际应用">实际应用</h5>

<p>但在很多任务中，累计误差下降到一定程度后，进一步下降会非常缓慢，这时标准BP算法往往会获得较好的解，尤其当训练集非常大时效果更明显。</p>

<h3 id="全局最与局部最了解">全局最⼩与局部最⼩：了解</h3>

<p>跳出局部最小的策略</p>

<ol>
  <li>多组不同的初始参数优化</li>
  <li>模拟退火技术</li>
  <li>随机梯度下降</li>
  <li>遗传算法</li>
</ol>

<h3 id="其他常见神经网络了解">其他常见神经网络：了解</h3>

<p>RBF网络：是哟个径向基函数$\rho(x,c_i)=e^{-\beta_i||x-c_i||^2}$作为激活函数</p>

<p>ART:竞争学习，无监督学习</p>

<p>SOM网络：竞争型无监督学习</p>

<p>级联相关网络：将网络结构也作为学习目标</p>

<p>Elman网络:递归神经网络,有反馈.</p>

<h3 id="深度学习了解">深度学习：了解</h3>

<p>深层神经网络最为典型</p>

<p>训练方法:预训练+微调. 预训练:每次训练一层,微调:对整个网络进行微调训练</p>

<p>权共享 (CNN)</p>

<p>理解深度学习:特征工程 VS 特征学习或表示学习</p>

<p>特征工程:手工设计特征</p>

<p>特征学习:通过深度学习自动产生分类的特征</p>

<h2 id="第六章-支持向量机-svm">第六章 支持向量机 SVM</h2>

<h3 id="间隔与支持向量">间隔与支持向量</h3>

<p>寻找超平面将样本划分，找最中间的超平面。
$
\mathop{\arg\max}<em>{w,b} \dfrac{2}{||w||}<br />
s.t.\quad y_i(w^Tx_i+b)\geq 1,i=1,2.,…,m<br />
\Leftrightarrow<br />
\mathop{\arg\min}</em>{w,b} \dfrac{||w||^2}{2}<br />
s.t.\quad y_i(w^Tx_i+b)\geq 1,i=1,2.,…,m
$</p>

<h3 id="对偶问题">对偶问题</h3>

<p>拉格朗日乘子法
$
L(w,b,\alpha)=\dfrac{1}{2}||w||^2-\sum\limits_{i=1}^{m}\alpha_i(y_i(w^Tx_i+b)-1)
$
令$L$对$w,b$的偏导为0可得
$
w=\sum\limits_{i=1}^m\alpha_iy_ix_i,\sum\limits_{i=1}^m\alpha_iy_i=0
$
回代
$
\min_\alpha\quad \dfrac{1}{2}\sum\limits_{i=1}^m\sum\limits_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j-\sum\limits_{i=1}^m\alpha_i<br />
s.t.\quad \sum\limits_{i=1}^m\alpha_iy_i=0<br />
\alpha_i\geq 0,i=1,2,…,m.
$
最终模型
$
f(x)=w^Tx+b=\sum\limits_{i=1}^m\alpha_iy_ix_i^Tx+b
$
KKT条件
$
\alpha_i\geq 0<br />
y_if(x_i)\geq 1<br />
\alpha_i(y_if(x_i)-1)=0
$</p>

<h4 id="求解smo">求解SMO</h4>

<p>基本思路：不断执行如下</p>

<ol>
  <li>选取一对需要更新的变量$\alpha_{i},\alpha_j$.</li>
  <li>固定$\alpha_i,\alpha_j$以外的参数，求解对偶问题更新$\alpha_i,\alpha_j$.注意仅考虑两个变量，约束也视为两个变量的约束，其他视为常数，显然有闭式解。</li>
</ol>

<p>b通过支持向量来计算。（对任意支持向量$x_s$，$y_s(\sum\limits_{i=1}^m\alpha_iy_ix_i^Tx_s+b)=1$）</p>

<p>支持向量机解的稀疏性：训练后，模型仅与支持向量有关，及$\alpha_i$不等于0的对应的向量有关。</p>

<h3 id="核函数">核函数</h3>

<p>当空间不线性可分时，将样本从低维映射到一个更高维的特征空间。</p>

<p>样本$x$映射后$\phi(x)$，超平面$f(x)=w^T\phi(x)+b$。在模型中仅以内积的形式出现。</p>

<p>Mercer定理(充分非必要)：只要一个对称函数所对应的核矩阵半正定, 则它就能作为核函数来使用.
$
k(x_i,x_j)=\phi(x_i)^T\phi(x_j)
$</p>

<h4 id="常用的核函数"><strong>常用的核函数</strong></h4>

<p><img src="C:\Users\赵超懿\AppData\Roaming\Typora\typora-user-images\image-20211225202119165.png" alt="image-20211225202119165" /></p>

<h3 id="软间隔与正则化">软间隔与正则化</h3>

<h4 id="软间隔">软间隔</h4>

<p>现实中, 很难确定合适的核函数使得训练样本在特征空间中线性可分; 同时一个线性可分的结果也很难断定是否是有过拟合造成的.</p>

<p>引入”软间隔”的概念, 允许支持向量机在一些样本上不满足约束.</p>

<p>基本想法：最大化间隔的同时, 让不满足约束的样本应尽可能少
$
\min_{w,b}\quad \dfrac{1}{2}||w||^2+C\sum\limits_{i=1}^ml_{0/1}(y_i(w^T\phi(x_i)+b)-1)
$
$l_{0/1}$是损失函数
$
l_{0/1}(z)=1, z &lt; 0
<br />
l_{0/1}(z)=0,otherwise
$
替代函数（因为0-1函数不连续）
$
l_{hinge(z)}=\max(0,1-z)
$
模型</p>

<p>原始问题
$
\mathop{\arg\min}<em>{w,b} \dfrac{||w||^2}{2}+C\sum\limits</em>{i=1}^m\max(0,1-y_i(w^T\phi(x_i)+b))<br />
s.t.\quad y_i(w^Tx_i+b)\geq 1,i=1,2.,…,m
$
对偶问题
$
\min_\alpha\quad \dfrac{1}{2}\sum\limits_{i=1}^m\sum\limits_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j-\sum\limits_{i=1}^m\alpha_i<br />
s.t.\quad \sum\limits_{i=1}^m\alpha_iy_i=0<br />
C\geq \alpha_i\geq 0,i=1,2,…,m.
$</p>

<h4 id="正则化">正则化</h4>

<p>$
\min_f\quad \Omega(f)+C\sum\limits_{i=1}^ml(f(x_i),y_i)
$</p>

<p>第一项是结构风险（描述模型的某些性质），第二项是经验风险（描述模型与训练数据的契合程度）</p>

<h3 id="支持向量回归">支持向量回归</h3>

<p>特点: 允许模型输出和实际输出间存在$2\epsilon$的偏差</p>

<p><img src="C:\Users\赵超懿\AppData\Roaming\Typora\typora-user-images\image-20211225203203092.png" alt="image-20211225203203092" /></p>

<p>损失函数</p>

<p><img src="C:\Users\赵超懿\AppData\Roaming\Typora\typora-user-images\image-20211225203230958.png" alt="image-20211225203230958" /></p>

<h3 id="核方法">核方法</h3>

<p><img src="C:\Users\赵超懿\AppData\Roaming\Typora\typora-user-images\image-20211225203439450.png" alt="image-20211225203439450" /></p>

<p>推广：核LDA（线性模型中提到），核PCA</p>

<h2 id="第七章-贝叶斯分类器">第七章 贝叶斯分类器</h2>

<h3 id="掌握贝叶斯决策论">掌握贝叶斯决策论</h3>

<p>给定N个类别，令$\lambda_{ij}$代表将第$j$类样本误分类为第$i$类所产生的损失，</p>

<p>将样本$x$分到第$i$类的条件风险为：
$
R(c_i|\mathbf{x})=\sum\limits_{j=1}^N\lambda_{ij}P(c_j|\mathbf{x})
$
贝叶斯判定准则
$
h^<em>(\mathbf{x})=\mathop{\arg\min}_{c\in Y}R(c|\mathbf{x})
$
$h^</em>$成为被贝叶斯最优分类器，总风险成为贝叶斯风险，反应学习性能的理论上限。</p>

<h4 id="判别式-vs-生成式">判别式 vs 生成式</h4>

<h5 id="判别式">判别式</h5>

<p>直接对$P(c|\mathbf{x})$建模如决策树，BP神经网络，SVM</p>

<h5 id="生成式">生成式</h5>

<p>先对$P(\mathbf{x},c)$建模，再由次获得$P(c|\mathbf{x})$.
$
P(c|\mathbf{x})=\dfrac{P(\mathbf{x},c)}{P(\mathbf{x})}=\dfrac{P(c)P(\mathbf{x}|c)}{P(\mathbf{x})}
$
代表：贝叶斯分类器，（贝叶斯分类器$\neq$贝叶斯学习）</p>

<h3 id="熟悉极大似然估计">熟悉极大似然估计</h3>

<p>先假设某种概率分布形式，再基于训练样例对参数进行估计</p>

<p>设$P(x|c)$具有某种概率分布，参数$\theta_c$.</p>

<p>$\theta_c$对训练集D中第c类样本组成的集合$D_c$的似然为
$
P(D_c|\theta_c)=\prod_{x\in D_c}P(x|\theta_c)
$
连乘容易下溢，故通常使用对数似然LL
$
LL(\theta_c)=\log{P(D_c|\theta_c)}=\sum_{x\in D_c}\log{P(x|\theta_c)}<br />
\hat{\theta}<em>c=\arg\max</em>{\theta_c}{LL(\theta)}
$</p>

<h3 id="熟悉朴素贝叶斯拉普拉斯修正">熟悉朴素贝叶斯（拉普拉斯修正）</h3>

<p>主要障碍：所有属性上的联合概率难以从有限训练样本估计获得</p>

<p>组合爆炸；样本稀疏</p>

<p>基本思路：假定属性独立
$
P(c|x)=\dfrac{P(c)P(x|c)}{P(x)}=\dfrac{P(c)}{P(x)}\prod_{i=1}^dP(x_i|c)
$</p>

<h4 id="估计">估计</h4>

<p>$P(c)=\dfrac{|D_c|}{|D|}$.</p>

<p>$P(x|c)$:</p>

<p>对离散属性，$D_{c,x_i}$表示$D_c$在第i个属性上取值为$x_i$的样本组成的集合，则$P(x_i|c)=\dfrac{|D_{c,x_i}|}{|D_c|}$.</p>

<p>对连续属性，考虑概率密度函数，假定$P(x_i|c)\sim N(\mu_{c,i},\sigma^2_{c,i})$.</p>

<h4 id="拉普拉斯修正">拉普拉斯修正</h4>

<p>若某个属性值在训练集中没有与某个类同时出现过，则直接计算会出现 问题，因为概率连乘将“抹去”其他属性提供的信息。</p>

<p>$P(c)=\dfrac{|D_c|+1}{|D|+N}$.$P(x_i|c)=\dfrac{|D_{c,x_i}|+1}{|D_c|+N_i}$.</p>

<h4 id="朴素贝叶斯分类器的使用">朴素贝叶斯分类器的使用</h4>

<p>若对预测速度要求高：预计算所有概率估值，使用时“查表”</p>

<p>若数据更替频繁：不进行任何训练，收到预测请求时再估值 (懒惰学习, lazy learning)</p>

<p>若数据不断增加：基于现有估值，对新样本涉及的概率估值进行修正 (增量学习, incremental learning)</p>

<h3 id="掌握半朴素贝叶斯">掌握半朴素贝叶斯</h3>

<p>基本思路：适当考虑一部分属性间的相互依赖信息</p>

<p>最常用策略：独依赖估计 (One-Dependent Estimator, ODE)</p>

<p>假设每个属性在类别之外最多仅依赖一个其他属性
$
P(c|x)\propto P(c)\prod_{i=1}^d P(x_i|c,pa_i)
$</p>

<h4 id="spode">SPODE</h4>

<p>假设所有属性都依赖于同一属性，称为“超父” (Super-Parent)， 然后通过交叉验证等模型选择方法来确定超父属性</p>

<h4 id="tan">TAN</h4>

<p>以属性间的条件”互信息”(mutual information)为边的权重，构建完全图，再利用最大带权生成树算法，仅保留强相关属性间的依赖性</p>

<h4 id="aode">AODE：</h4>

<p>尝试将每个属性作为超父构建 SPODE</p>

<p>将拥有足够训练数据支撑的 SPODE 集成起来作为最终结果
$
P(c|x)\propto \sum_{i=1,|D_{x_i}|\geq m’}^d P(c,x_i)\prod_{j=1}^d P(x_j|c,x_i)<br />
\hat{P}(c,x_i)=\dfrac{|D_{c,x_i}|+1}{|D|+N_i}<br />
\hat{P}(x_j|c,x_i)=\dfrac{|D_{c,x_i,x_j}|+1}{|D_{c,x_i}|+N_j}
$</p>

<h4 id="高阶依赖">高阶依赖</h4>

<p>需要充分的样本</p>

<h3 id="掌握贝叶斯网">掌握贝叶斯网</h3>

<h3 id="了解em算法">了解EM算法</h3>

<h2 id="第八章-集成学习">第八章 集成学习</h2>

<h3 id="个体与集成知道个体分类器的定义和集成学习的定义">个体与集成：知道个体分类器的定义和集成学习的定义</h3>

<p>集成学习通过构建并结合多个学习器来提升性能</p>

<h3 id="boosting知道boosting的思想和adaboost的实现">Boosting：知道Boosting的思想和adaboost的实现</h3>

<h4 id="boosting">Boosting</h4>

<p>个体学习器存在强依赖关系，串行生成，每次调整训练数据样本分布</p>

<h4 id="adaboost">AdaBoost</h4>

<p><img src="C:\Users\赵超懿\AppData\Roaming\Typora\typora-user-images\image-20211226162942021.png" alt="image-20211226162942021" /></p>

<p>($Z_t$是规范化因子，保证权重之和是1)</p>

<p>基学习器的线性组合
$
H(x)=\sum_{t=1}^T\alpha_th_t(x)
$
最小化指数损失函数
$
l_{exp}(H|D)=E_{x\sim D}[e^{-f(x)H(x)}]
$</p>

<h5 id="注意事项">注意事项</h5>

<p>数据分布的学习：重赋权法，重采样法</p>

<p>重启动，避免训练过程过早停止，（8.5处，如果学习的分类器不佳）</p>

<h3 id="bagging与随机森林知道思想和实现的方式">Bagging与随机森林：知道思想和实现的方式</h3>

<p>个体学习器不存在强依赖关系</p>

<p>并行化生成</p>

<p>自助采样法</p>

<h4 id="bagging算法">Bagging算法</h4>

<p><img src="C:\Users\赵超懿\AppData\Roaming\Typora\typora-user-images\image-20211226164842877.png" alt="image-20211226164842877" /></p>

<p>($D_{bs}$是自助采样产生的样本分布)</p>

<p>时间复杂度低 :</p>

<ol>
  <li>
    <p>假定基学习器的计算复杂度为O(m)，采样与投票/平均过程的复杂度 为O(s)，则bagging的复杂度大致为T(O(m)+O(s))</p>
  </li>
  <li>
    <p>由于O(s)很小且T是一个不大的常数</p>
  </li>
  <li>
    <p>因此训练一个bagging集成与直接使用基学习器的复杂度同阶</p>
  </li>
</ol>

<p>可使用包外估计</p>

<h4 id="包外估计">包外估计</h4>

<p>可以计算泛化误差等</p>

<h4 id="随机森林">随机森林</h4>

<p>采样的随机性，属性的随机性</p>

<p><img src="C:\Users\赵超懿\AppData\Roaming\Typora\typora-user-images\image-20211226165628669.png" alt="image-20211226165628669" /></p>

<h3 id="结合策略知道集中常用策略以及stacking的优缺点">结合策略：知道集中常用策略以及stacking的优缺点</h3>

<p>平均法：如加权平均法</p>

<p>投票法：绝对多数投票法和相对多数投票法</p>

<p>学习法：Stacking 先从初始数据集训练出初级学习器，然后”生成”一个新数据集用于训练次级学习器.</p>

<p>在训练阶段，次级训练集是利用初级学习器产生的，若直接用初级学习器 的训练集来产生次级训练集，则过拟合风险会比较大;因此一般是通过使用交叉验证或留一法这样的方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本.</p>

<h3 id="多样性知道多样性扰动的几种办法">多样性：知道多样性扰动的几种办法</h3>

<h4 id="误差-分歧分解">误差-分歧分解</h4>

<p>$
E=\bar{E}-\bar{A}
$</p>

<p>($\bar{E}$表示个体学习器泛化误差的加权评分，$\bar{A}$表示个体学习器的加权分歧值。)</p>

<p>个体学习器精确性越高、多样性越大，则集成效果越好</p>

<h4 id="多样性增强方法">多样性增强方法</h4>

<p>数据样本扰动：</p>

<ol>
  <li>采样</li>
</ol>

<p>输入属性扰动：</p>

<ol>
  <li>随机子空间算法</li>
</ol>

<p>输出表示扰动：</p>

<ol>
  <li>翻转法：随机改变输入样本的标记</li>
  <li>输出调剂法：分类输出改为回归输出得到分类器</li>
  <li>ECOC法：多类任务分解为一系列两类任务来求解</li>
</ol>

<p>算法参数扰动：</p>

<ol>
  <li>负相关法：强制要求个体神经网络采用不同的参数</li>
  <li>不同的多样性增强机制同时使用</li>
</ol>

:ET